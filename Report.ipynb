{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karl Task Solver using Policy Gradient Algorithms\n",
    "This projects aims at solving any generic 4x4 Karel task using policy gradient algorithms in an optimal manner (least sequence of actions length). \n",
    "# Classes\n",
    "## Karel(gym.Env):\n",
    "This is the main class which represents the Karel task as a gym environemnt. Karel inherits from `gym.Env`.\n",
    "### Constructor Input:\n",
    "1. `paths` list(str): a list of string paths for the Karel tasks in the dataset.\n",
    "2. `state_space` (str): default \"S0\": can have values \"S1\" and \"S2\" to change the binary state representation.\n",
    "3.  `conssequential_loading`: choose whether to load tasks sequentially as ordered in their directory or randomly sampled.\n",
    "### Important Functions \n",
    "1. `reset(Self):` increpement the `current_ep` index and returns the new task the state is pointing at. Should be called at the beginning once after the initalization. \n",
    "2. `reward(self, human_state, action):` has the following format:\n",
    "```\n",
    "        if action == Action.finish.value:\n",
    "            if cur_grid == post_grid:\n",
    "                self.solved = True\n",
    "                return 1\n",
    "        elif self.CRASH:\n",
    "            return -1\n",
    "        return 0\n",
    "```\n",
    "2. `step(self, action):` takes action returns: next state, reward, done (boolean), info (dict showing information)\n",
    "3. `render(self, mode='console'):` prints the current environemnt (state). Supports `console` option. \n",
    "## PolicyGradientBase\n",
    "This is the base class for all policy gradients algorithms. It abstract non-algorithm specific functionality that are common between them. Whenever this parent class is classes by its two child classes (SAC and A2C), the following parameters are to be passed:\n",
    "\n",
    "1. env: a gym environemnt object\n",
    "2. network: MLPA2C or MLPSAC network (ideally can take any PyTorch network with certain specifications.\n",
    "3. gamma (default: 0.99): the discount factor\n",
    "4. lr (default: 03e-4): learning rate\n",
    "5. evaluate_every_n: (default: 500): evaluate after iterating over n episodes\n",
    "6. show_every_n (default:100): rendering option during evaluation. # not recommended for speed\n",
    "7. alpha=0.5: step size for actor and critic updates\n",
    "8. update_every_n =5: update after storing n episodes in the replay_buffer\n",
    "9. show_every_n (default:100): rendering option during evaluation. # not\n",
    "10. epochs = 5: the number of rounds over the entire dataset \n",
    "11. num_actions=6: action diemnsion \n",
    "12. learn_by_demo=True: set to learn by demonstration (i.e, get actions from expert dataset)\n",
    "13. load_model=False: set to load pre-trained model (Expected ot be in `models/dir`\n",
    "### Functions (major ones)\n",
    "1. `train(self, dataset_dict, dataset_all, expert_trajectories=None):` trains the policy passed to the class during construction using the provided dataset, evaluates it. Uses expert_trajectories if `learn_by_demo` is set.\n",
    "2. `eval_model(self, ds):` takes a dataset and evaluates the model on it. Returns a dict of evaluation stats.\n",
    "3. `load_algo_policy(self):`: loads the state dictionary for the policy. Should be in 'stats' folder with the same name of the model.\n",
    "4. `gen_episode(self, obs, optimal_actions):` generates episode rollout and stores in `self.replay_buffer`.\n",
    "5. `load_overall_stats(self):`: loads model dumped stats from `stats/{model_name}`\n",
    "6. `save_overall_stats(self):` saves model overall stats (trainiing) in `stats/{model_name}`\n",
    "7. `save_algo_policy(self):` save algorithm policy state dict.\n",
    "## A2C(PolicyGradientBase):\n",
    "This class implement Advantage Actor Critc with support for learn by demonstration for a discrete action space. It also support batched updates specified by `update_every_n` to update every certain number of episodes stored in the replay buffer. \n",
    "It has two main attributes:\n",
    "1. Policy (Actor) Optimizer: self.Poptim\n",
    "2. Value (Critic) Optimizer: self.Voptim\n",
    "### Functions\n",
    "1. The policy $\\pi$ (Actor) loss  is computed as the following:\n",
    "```\n",
    "    log_pi, G, v = replay_buffer.logps, replay_buffer.Gs, replay_buffer.vs\n",
    "    advantage = G - v.detach() if not self.learn_by_demo else 1\n",
    "    if self.clip_range:\n",
    "        c_lo, c_hi = self.clip_range\n",
    "        pi_comp = torch.min(log_pi, log_pi.clamp(min=c_lo, max=c_hi))\n",
    "        loss_pi = self.alpha * (-pi_comp*advantage).mean()\n",
    "    else:\n",
    "        loss_pi = self.alpha * (-log_pi*advantage).mean()\n",
    "    return loss_pi\n",
    "```\n",
    "2. The value $\\hat{v}$ (Critic) loss  is computed as the following:\n",
    "```\n",
    "    G, v = replay_buffer.Gs, replay_buffer.vs\n",
    "    loss_v= self.alpha*((G - v).pow(2)).mean()\n",
    "    return loss_v\n",
    "```\n",
    "3. `update_agent(self):` perform a backward for both actor and critic's networks.\n",
    "## SAC(PolicyGradientBase):\n",
    "This class implements the Soft Actor Critic with support for learning by demonstration if expert trajectories were supplied.\n",
    "It has two main attributes:\n",
    "1. Policy (Actor) Optimizer: self.Poptim\n",
    "2. Q-value (Critic) Optimizer: self.Qoptim\n",
    "### functions:\n",
    "1. The policy $\\pi$ (Actor) loss  is computed as the following:\n",
    "```\n",
    "s, a =replay_buffer.states, replay_buffer.actions\n",
    "q1_pi = self.network.q1(s,a)\n",
    "q2_pi = self.network.q2(s,a)\n",
    "q_pi = torch.min(q1_pi, q2_pi)\n",
    "\n",
    "loss_pi = (-self.alpha * replay_buffer.logps - q_pi).mean()\n",
    "```\n",
    "2. The value $\\hat{Q}$ (Critic) loss  is computed as the following:\n",
    "```\n",
    "    s, a, r, s_next,  d = replay_buffer.states, replay_buffer.actions, replay_buffer.rewards, replay_buffer.states_next, replay_buffer.donez\n",
    "    # Current Qs\n",
    "    q1 = self.network.q1(s,a)\n",
    "    q2 = self.network.q2(s,a)\n",
    "    with torch.no_grad():\n",
    "        a2, (logp_a2, _) = self.network.pi.predict(s_next, deterministic=True)\n",
    "        t_q1 = self.target_network.q1(s_next, a2)\n",
    "        t_q2 = self.target_network.q2(s_next, a2)\n",
    "        q_pi_targ = torch.min(t_q1, t_q2)\n",
    "        bellman = r + self.gamma * (1 - d) * (q_pi_targ - self.alpha * logp_a2)\n",
    "\n",
    "    # MSE loss against Bellman backup\n",
    "    loss_q1 = ((q1 - bellman)**2).mean()\n",
    "    loss_q2 = ((q2 - bellman)**2).mean()\n",
    "    loss_q = loss_q1 + loss_q2\n",
    "```\n",
    "# Other directories\n",
    "1. `stable_baselines_experiments/:` contains the main pytohn script for experiments done on agents from stable baselines project (ACER, A2C, PPO1). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code with Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from algorithms.sac import SAC\n",
    "from algorithms.a2c import A2C\n",
    "from networks.mlpsac import MLPSAC\n",
    "from networks.mlpa2c import MLPA2C\n",
    "from stable_baselines3.common import policies\n",
    "from utils import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameeters and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:01<00:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved  trajectories/data_easy_S0_traj.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:03<00:02,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved  trajectories/data_medium_S0_traj.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved  trajectories/data_S0_traj.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved  trajectories/all_S0_traj.pkl\n"
     ]
    }
   ],
   "source": [
    "dataset_dict, dataset_all = load_datasets(dataset_dir=\"dataset\")\n",
    "hidden_acts_a2c = {\n",
    "    \"pi\":nn.ReLU,\n",
    "    \"v\": nn.ReLU\n",
    "}\n",
    "archs = { \n",
    "    'SAC':{\n",
    "        \"pi\": [256, 256],\n",
    "        \"q\" : [256, 256]\n",
    "    }, \n",
    "    'SAC with Demo (512, 256)':{\n",
    "        \"pi\": [512, 256],\n",
    "        \"q\" : [512, 256]\n",
    "    }, \n",
    "    'A2C':{\n",
    "        \"pi\": [256, 256],\n",
    "        \"v\" : [256, 256]\n",
    "    },   \n",
    "    'A2C with Demo (512, 256)':{\n",
    "        \"pi\": [512, 256],\n",
    "        \"v\" : [512, 256]\n",
    "    }\n",
    "}\n",
    "hidden_acts_sac = {\n",
    "    \"pi\":nn.ReLU,\n",
    "    \"q\": nn.ReLU\n",
    "}\n",
    "hidden_acts_a2c = {\n",
    "    \"pi\":nn.ReLU,\n",
    "    \"v\": nn.ReLU\n",
    "}\n",
    "state_type = 'S0'\n",
    "dataset_dict, dataset_all = load_datasets(dataset_dir=\"dataset\")\n",
    "expert_trajectories = dataset_to_trajs(dataset_dict, save_path=\"trajectories\", traj_obj=False, state_space=state_type)\n",
    "env = Karel(dataset_all['train_task'], state_type, sequential_loading=False)\n",
    "state_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning Experiment (best model: BC-FFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_reward': 0.20027979196728923, 'avg_extra_steps': 0.00925497454881999, 'solved_percentage': 0.9004166666666666, 'solved_optimally_percentage': 0.8975, 'avg_disc_returns': 0.8644595549614149}\n"
     ]
    }
   ],
   "source": [
    "arch = [128,128, 128]\n",
    "class FeedForwardPolicy(policies.ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs, net_arch=arch)\n",
    "best_model_path = \"models/FeedForward_S0_[128, 128, 128]_15_32_0.89125_0.8791666666666667.zip\"\n",
    "policy = bc.reconstruct_policy(best_model_path)\n",
    "stats = eval_model(policy, dataset_dict['data'], state_type=\"S0\", show_every_n=1e10)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C Best Model: Learning with Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_reward': 0.19575117243866955, 'avg_extra_steps': 0.01810385898046689, 'solved_percentage': 0.8745833333333334, 'solved_optimally_percentage': 0.8683333333333333, 'avg_disc_returns': 0.8399508973367106}\n"
     ]
    }
   ],
   "source": [
    "for experiment in ['A2C with Demo (512, 256)']:\n",
    "    network = MLPA2C(state_size, n_actions, archs[experiment], hidden_acts_a2c, device)\n",
    "    policy = A2C(env,network=network,model_name=experiment)\n",
    "    policy.load_algo_policy()\n",
    "    stats = eval_model(policy.network.pi, dataset_dict['data'], state_type=\"S0\", show_every_n=1e10)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC Best Model: Learning with Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_reward': 0.19601603835978543, 'avg_extra_steps': 0.007655502392344498, 'solved_percentage': 0.8708333333333333, 'solved_optimally_percentage': 0.8679166666666667, 'avg_disc_returns': 0.8373242925687909}\n"
     ]
    }
   ],
   "source": [
    "for experiment in ['SAC with Demo (512, 256)']:\n",
    "    network = MLPSAC(state_size, n_actions, archs[experiment], hidden_acts_sac, device)\n",
    "    policy = SAC(env,network=network,model_name=experiment)\n",
    "    policy.load_algo_policy()\n",
    "    stats = eval_model(policy.network.pi, dataset_dict['data'], state_type=\"S0\", show_every_n=1e10)\n",
    "    print(stats)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1847589117051cc24c85377a0cce86108c4f37d3b05896fc24858d9e4677c68e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('RL': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
